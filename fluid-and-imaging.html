<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Biostatistics for Fluid and Imaging Biomarkers</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.27/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/styles.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: middle, center

# Biostatistics for Fluid and Imaging Biomarkers

Michael Donohue, PhD

University of Southern California

### Biomarkers for Neurodegenerative Disorders

BarcelonaBeta Brain Research Center

May 2024


  




.pull-left[

&lt;img src="./images/atri.png" width="57%"  style="display: block; margin: auto;" /&gt;

]


.pull-right[

&lt;img src="./images/actc_logo.png" width="47%"  style="display: block; margin: auto;" /&gt;

]

---

# Course Overview

.large[
Topics:

- Hour 1 -- Biostatistics for Fluid and Imaging Biomarkers
- Hour 3 -- Modeling Longitudinal Data (Lars Racket)

Emphases:

- Visualization 
- Demonstrations using R, code available from:
  - [https://github.com/atrihub/biomarkers-neuro-disorders](https://github.com/atrihub/biomarkers-neuro-disorders)
]

---

# Session 1 Outline (Fluid)

.large[
- Batch Effects
- Experimental Design (Sample Randomization)
- Statistical Models for Assay Calibration/Quantification
- Classification (Supervised Learning)
  - Logistic Regression
  - Binary Trees
  - Random Forest
- Mixture Modeling (Unsupervised Learning)
  - Univariate
  - Bivariate
- Mixture of Experts (Unsupervised Learning with covariates)
]

---

# Session 1 Outline (Imaging)

.large[
- Reference Regions
- Centiloids
- Harmonization using the Empirical Cumulative Distribution Function (ECDF)
- Intro to longitudinal data analysis
  - Summaries and plots
  - Regression
  - ANCOVA
  - Two-stage models
]

---

class: inverse, middle, center

# Batch Effects

---

# Batch Effects: Boxplot



&lt;img src="fluid_fig/batch_data_plot-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Coefficient of Variation

.pull-left[

&lt;table class="table table-striped table-condensed" style="font-size: 18px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; batch &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; N &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Mean &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SD &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SD/Mean = CV (%) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 790 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 379 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 48 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 925 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 299 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 725 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 389 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 54 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 951 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 332 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 35 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 690 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 312 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 45 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 867 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 349 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 40 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 837 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 446 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 53 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 914 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 348 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 38 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 883 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 271 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 31 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 763 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 266 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 35 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



]

.pull-right[

- Coefficient of Variation (CV) = SD/Mean
- Often used for quality control (reject batch with CV &gt; `\(x\)`)

]

---

# Testing for Batch Effects


``` r
anova(lm(Biomarker ~ batch, batch_data))
Analysis of Variance Table

Response: Biomarker
           Df   Sum Sq Mean Sq F value  Pr(&gt;F)    
batch       9  3573109  397012    3.37 0.00051 ***
Residuals 490 57758046  117874                    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

* Batch explains a significant amount of the variation in this simulated data
* R note: `batch` variable must be a `factor`, not `numeric` (otherwise, you will get a batch slope)

---

# Batch effects: Confounds

&lt;img src="fluid_fig/batch_confounds-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

Suppose we have groups of interest (say, active vs placebo) that we would like to compare.

Do we see an problem here?

---

class: inverse, middle, center

# Experimental Design for Fluid Biomarkers

---

# Randomized assignment of samples to plates

&lt;img src="fluid_fig/batch_randomized-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

If we have both groups represented in each batch, we can disentangle batch effects
and group effects

One way to ensure this, is to randomize samples to batches

---

# Experimental Design for Fluid Biomarkers

.large[
- Randomize samples to batches/plates
- Longitudinally collected samples (samples collected over time on same individual):
  - If batch effects are expected to be larger than storage effects, consider randomizing *individuals* to batches (i.e. keep all samples from individual on the same plate)
  - However, if storage effects are a concern, timely sample processing might be preferred.
- Randomization can be stratified to ensure important factors (e.g. treatment group, age, APOE `\(\epsilon4\)`) are balanced over batches.
]

---

# Sample Randomization

We use an `R` package [SRS](https://github.com/atrihub/SRS) ("Subject Randomization System"), which we have modified to deal with the constraints of plate capacity, and keeping samples from the same subject together.

(Note this is different than the `SRS` package on CRAN)



&lt;table class="table table-striped table-condensed" style="font-size: 18px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Subject ID &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Num. of samples &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Group &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Age &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Plate &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; old &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; old &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; old &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; young &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; old &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; young &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; young &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; old &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; young &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; old &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 12 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



---

# Sample Randomization

.pull-left[

&lt;table class="table table-striped table-condensed" style="font-size: 18px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Plate &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; old &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; young &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Num. samples &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 29 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 29 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 30 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 30 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 30 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 29 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 30 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



]

.pull-right[

- Number of young and old well balanced across the 13 plates
- Number of samples per plate is also reasonable (plate capacity was set at 30 samples)

]

---

class: inverse, middle, center

# Calibration

---

# Calibration

.large[

- Calibration: developing a map from "raw" assay responses to concentrations (ng/ml) using samples of *known* concentrations
- We will explore some approaches to calibration with methods from the `R` package `calibFit` (Haaland et al., 2011; Davidian et al., 1990)
- The package includes some example data:
  - High Performance Liquid Chromatography (HPLC) and 
  - Enzyme Linked Immunosorbent Assay (ELISA)
- These examples are taken straight from the package vignette

]

???

The package is not actively maintained, so you must install the package from the CRAN archive

---

# Calibration

.pull-leftWider[

&lt;img src="fluid_fig/calibFit_fits-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

]

.pull-rightNarrower[

- *Calibration* is *inverse regression* in which these fitted curves would be used to map assay responses from samples of unkown concentration (vertical axis) to concentration values (horizontal axis).
- Both fits exhibit *heteroscedasticity*: the error variance is not constant with respect to Concentration
- Most models assume *homoscedasticity*, or constant error variance.

]

---

# Residuals (difference between response &amp; fitted values)

&lt;img src="fluid_fig/calibFit_residuals-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Typical Regression

Typically, regression models are of the form: 

`\begin{equation}
Y_{i}=f(x_i,\beta)+\epsilon_{i}, 
\end{equation}`

where:

- `\(Y_{i}\)` is the observed response/outcome for `\(i\)`th individual ( `\(i=1,\ldots,n\)` ) 
- `\(x_i\)` are covariates/predictors for `\(i\)`th individual
- `\(\beta\)` are regression coefficients to be estimated
- `\(f(\cdot,\cdot)\)` is the model (assumed "known" or to be estimated)
  - In linear regression `\(f(x_i,\beta)=x_i\beta\)`
- `\(\epsilon_i\)` is the residual error
- We assume `\(\epsilon\sim\mathcal{N}(0,\sigma^2)\)` 
- `\(\sigma\)` is the *constant* standard deviation (*homoscedastic*)

If the standard deviation is not actually constant (*heteroscedastic*), estimates might be unreliable.


---

## Ordinary Least Squares: minimizing the sum of squared residuals

&lt;video width="100%"  controls loop&gt;&lt;source src="fluid_fig/regression-movie.webm" /&gt;&lt;/video&gt;

`\(^*\)` RSS = Residual sum of squares, or `\(\sum_i (\textrm{Observed}_i-\textrm{Fitted}_i)^2\)`

---

# Modeling Heteroscedastic Errors

The `calibFit` package includes models of the form: 

`\begin{equation}
Y_{ij}=f(x_i,\beta)+\sigma g(\mu_i,z_i,\theta) \epsilon_{ij}, 
\end{equation}`

where,

- `\(Y_{ij}\)` are observed assay values/responses for `\(i\)`th individual ( `\(i=1,\ldots,n\)` ), `\(j\)`th replicate
- `\(g(\mu_i,z_i,\theta)\)` is a function that allows the variances to depend on:
  - `\(\mu_i\)` (the mean response `\(f(x_i,\beta)\)`), 
  - covariates `\(z_i\)`, and 
  - a parameter ("known" or unknown) `\(\theta\)`.
- `\(\epsilon_{ij}\sim\mathcal{N}(0,1)\)` 

In particular, `calibFit` implements the Power of the Mean (POM) function

`\begin{equation}
g(\mu_i,\theta) = \mu_i^{2\theta}
\end{equation}`

which results in 

`\begin{equation}
\operatorname{var}(Y_{ij}) = \sigma^2\mu_i^{2\theta}
\end{equation}`

???

allowing the variance to depend on the mean.

---

# "Homogenized" Residuals From Fits with POM



&lt;img src="fluid_fig/calibFit_pom_residuals-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# HPLC Calibration With/Without POM Variance

&lt;img src="fluid_fig/calib_hplc_pom-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

The mean does not change much, but we get more accurate and tighter 95% confidence bands

---

# Elisa Calibration With/Without POM Variance

&lt;img src="fluid_fig/calib_elisa_pom-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Calibrated Estimates for Each Sample

.pull-left[

&lt;img src="fluid_fig/calibrated1-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

]

.pull-right[

&lt;img src="fluid_fig/calibrated2-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

]

???

* MDC is Minimum Detectable Concentration, which we'll define on the next slide

---

# Calibration Statistics

Assuming calibration curve `\(f\)`, mapping concentrations to assay responses, is increasing, we define the following terms.

**Minimum Detectable Concentration (MDC)**: The lowest concentration where the curve is increasing, or:

  `$$x_{\textrm{MDC}} = \min\{x : f(x, \beta) &gt; \textrm{UCL}_0\}$$`
  
  where `\(\textrm{UCL}_0\)` is the upper confidence limit at 0

**Reliable Detection Limit (RDL)**: The lowest concentration that has a high probability of producing a response that is significantly greater than the response at 0, or 
  
`$$x_{\textrm{RDL}} = \min\{x : \textrm{LCL}_x &gt; \textrm{UCL}_0 \}$$`

**Limit of Quantitization (LOQ)**: The lowest concentration at which the coefficient of variation is less than a fixed percent (default is 20% in the `calibFit` package).

---

class: inverse, middle, center

# Supervised Learning

## Classification

---

# Classification

.pull-leftWider[

&lt;img src="fluid_fig/classification-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

]

.pull-rightNarrower[

- Data from [adni.loni.usc.edu](adni.loni.usc.edu)
- CSF Abeta 1-42 and t-tau assayed using the automated Roche Elecsys and cobas e 601 immunoassay analyzer system
- Filter time points associated with first assay, and ignore subsequent time points
- We'll ignore MCI and focus on CN vs Dementia
- Values greater than the upper limit of detection have been assigned the limit

]

---

# Classification

&lt;img src="fluid_fig/classification_no_mci-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Reciever Operatoring Characteristic (ROC) Curves

.pull-left[

&lt;img src="fluid_fig/ROC_abeta-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

]

.pull-right[

For each potential threshold applied to CSF `\(\textrm{A}\beta 42\)`, 
we calculate:
- Sensitivity: True Positive Rate = TP/(TP+FN)
- Specificity: True Negative Rate = TN/(TN+FP)

This traces out the ROC curve.

A typical summary of a classifier's performance is the
Area Under the Curve (AUC)

AUC=0.83 in this case, with 95% CI ( 0.8, 0.86 )

AUCs close to one indicate good performance.

The threshold shown here maximizes the distance between the curve
and the diagonal line (chance) (Youden, 1950)

]

???

Sensitivity is a measure of how well we are detecting positive cases

Specificity is a measure of how well we are detecting controls or negative cases

Youden's index gives equal weight to false positives and false negatives (not necessarily appropriate)

---

# Comparing ROC Curves

.pull-left[

&lt;img src="fluid_fig/ROC_abeta_tau-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

]

.pull-right[


| Marker                 | AUC                  | 95% CI                       | P-value `\(^*\)` |
| ---------------------- |:--------------------:| ----------------------------:| ------------:|
| `\(\textrm{A}\beta\)`      | 0.83    | 0.8, 0.86    |              |
| Tau                    | 0.78      | 0.75, 0.82      |  0.07        |
| Tau/ `\(\textrm{A}\beta\)` | 0.9 | 0.87, 0.92 |  &lt;0.001      |
`\(^*\)` Bootstrap test comparing each row to `\(\textrm{A}\beta\)` (Robin et al., 2011)

So the ratio of Tau / `\(\textrm{A}\beta\)` shows the best discrimination of NC from Dementia cases.



]

---

# Youden's Cutoff for Tau / `\(\textrm{A}\beta\)` Ratio

&lt;img src="fluid_fig/abeta_tau_scatter_youden-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

Line is Tau = 0.394 `\(\times\)` Abeta, depicting Youden's cutoff (maximizes sensitivity + specificity)

???

Youden's cutoff maximizing sensitivity + specificity is appropriate if sensitivity and specificity are equally important

---

# Logistic Regression



|Coefficient  | Estimate| Std. Error| z value|Pr(&gt;&amp;#124;z&amp;#124;) |
|:------------|--------:|----------:|-------:|:------------------|
|(Intercept)  |    -0.89|       0.13|    -6.7|&lt;0.001             |
|scale(ABETA) |    -1.59|       0.15|   -10.6|&lt;0.001             |
|scale(TAU)   |     1.26|       0.14|     9.0|&lt;0.001             |



`$$\log\big(\frac{p}{1-p}\big) = \hat\gamma_0 + A\beta_z \hat\gamma_{A\beta} + \textrm{tau}_z \hat\gamma_{\textrm{tau}}$$`
where `\(\hat\gamma\)` are regression coefficients.

---

# Logistic Regression Predicted Probabilities

&lt;img src="fluid_fig/logistic_pred_prob-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

line again depicts Youden's cutoff

---

# Ratio Contours

&lt;img src="fluid_fig/ratio_gradient-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

by using ratio's we're simplifying the bivariate scatter by assuming all dots along
these lines intersecting (0,0) are equivalent

dashed line has slope 1
---


# Logistic Regression Predicted Probability Contours

&lt;img src="fluid_fig/ratio_gradient_logistic-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

in contrast, logistic regression assumes the predicted probability gradient follows these
parallel lines

lines now are where predicted probabilities from logistic regression are constant

---

# Comparing ROC Curves

.pull-left[

&lt;img src="fluid_fig/ROC_logistic-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

]

.pull-right[


| Marker                 | AUC                      | 95% CI                           | P-value `\(^*\)` |
| ---------------------- |:------------------------:| --------------------------------:| ------------:|
| `\(\textrm{A}\beta\)`      | 0.83        | 0.8, 0.86        |              |
| Tau                    | 0.78          | 0.75, 0.82          |  0.07        |
| Tau/ `\(\textrm{A}\beta\)` | 0.9     | 0.87, 0.92     |  &lt;0.001      |
| Logisitic model        | 0.9 | 0.87, 0.92 |  &lt;0.001      |
`\(^*\)` Bootstrap test comparing each row to `\(\textrm{A}\beta\)` (Robin et al., 2011)

Logistic model ROC is very similar to Tau/ `\(\textrm{A}\beta\)` ratio ROC.



]

---

# Logistic Regression with Age and APOE



|Coefficient              | Estimate| Std. Error| z value|Pr(&gt;&amp;#124;z&amp;#124;) |
|:------------------------|--------:|----------:|-------:|:------------------|
|(Intercept)              |    -1.12|       0.17|    -6.5|&lt;0.001             |
|scale(ABETA)             |    -1.43|       0.16|    -9.0|&lt;0.001             |
|scale(TAU)               |     1.19|       0.14|     8.5|&lt;0.001             |
|scale(I(AGE + Years.bl)) |     0.14|       0.12|     1.2|0.230              |
|as.factor(APOE4)1        |     0.37|       0.25|     1.5|0.144              |
|as.factor(APOE4)2        |     1.26|       0.45|     2.8|0.005              |





This model does not provide much better ROC, either.

---

# Regression Trees

&lt;img src="fluid_fig/tree1-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

Hothorn et al. (2006)

???

Regression trees use recursive partitioning to classify data into more and more homogeneous subgroups

---

# Tree-based Methods

&lt;img src="fluid_fig/tree2-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

With this shallow tree, we end up with these four partitions of the Abeta-by-Tau scatter

---

# Comparing ROC Curves

.pull-left[

&lt;img src="fluid_fig/ROC_rf-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

]

.pull-right[


| Marker                 | AUC                      | 95% CI                           | P-value `\(^*\)` |
| ---------------------- |:------------------------:| --------------------------------:| ------------:|
| `\(\textrm{A}\beta\)`      | 0.83        | 0.8, 0.86        |              |
| Tau                    | 0.78          | 0.75, 0.82          |  0.07        |
| Tau/ `\(\textrm{A}\beta\)` | 0.9     | 0.87, 0.92     |  &lt;0.001      |
| Logisitic model        | 0.9 | 0.87, 0.92 |  &lt;0.001      |
| Binary Tree            | 0.88     | 0.86, 0.91     |  &lt;0.001      |
| Random Forest          | 0.95       | 0.93, 0.96       |  &lt;0.001      |
`\(^*\)` Bootstrap test comparing each row to `\(\textrm{A}\beta\)` (Robin et al., 2011)

Random Forests (Breiman, 2001; Hothorn et al., 2006) re-fit binary trees on random subsamples of the data, then aggregate resulting trees into a "forest". This results in smoother predictions and a smoother ROC curve.

* All three models should be (cross) validated, since they learn from known Dx.



]

---

class: inverse, middle, center

# Unsupervised Learning

## Mixture Modeling

---

# Unsupervised Learning

.large[
- The classification techniques we just reviewed can be thought of as *Supervised Learning* in which we attempt to learn known "labels" (CN, Dementia).
- *Mixture Modeling* is type of *Unsupervised Learning* technique in which we try to identify clusters of populations which appear to be arising from different distributions
- Don't confuse *Mixture Models* with *Mixed-Effects Models* (which we'll discuss later)
  - Think: "Mixture of Distributions"
]

---

# Distribution of ABETA

&lt;img src="fluid_fig/density_Abeta-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

- Distribution is bimodal
- Can we identify the two sub-distributions?
- We'll explore with `mixtools` package (Benaglia et al., 2009)

---

# Distribution of ABETA



&lt;img src="fluid_fig/mixture_distribution_Abeta-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

mixture models provide latent class membership probabilities, such as these 

---

# Posterior Membership Probabilities



| Abeta| Prob. Abnormal| Prob. Normal|
|-----:|--------------:|------------:|
|  1033|           0.58|         0.42|
|  1036|           0.57|         0.43|
|  1044|           0.53|         0.47|
|  1048|           0.52|         0.48|
|  1061|           0.46|         0.54|
|  1071|           0.42|         0.58|
|  1071|           0.42|         0.58|
|  1072|           0.41|         0.59|



---

## Bivariate Density



&lt;iframe src="bvdensity_csf_tau.html" width="100%" height="500" id="igraph" scrolling="no" seamless="seamless" frameBorder="0"&gt; &lt;/iframe&gt;

---

# Bivariate Density Contour Plot

&lt;img src="fluid_fig/bv_kernel_density-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Bivariate Mixture Model Posterior Probabilities

.pull-left[
&lt;img src="fluid_fig/mvmix_post_prob-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="fluid_fig/mvmix_density-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;
]

???

line on the left is still Youden's cutoff for the ratio

Contours are confidence ellipses at 99%, 95%, and 90% just to help see the shape of the estimated distributions

---

# Mixture of Experts (unsupervised learning with covariates)

&lt;img src="./images/moe.png" width="60%"  style="display: block; margin: auto;" /&gt;

Murphy et al. (2020); Murphy and Murphy (2022)

---

# Mixture of Experts (unsupervised learning with covariates)

&lt;img src="fluid_fig/unnamed-chunk-15-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

MAP = maximum a posteriori classification

---

# Summary of considerations for fluid biomarkers

.large[
- Batch Effects
- Experimental Design (Sample Randomization)
- Statistical Models for Assay Calibration/Quantification
- Classification (Supervised Learning)
  - Logistic Regression
  - Binary Trees
  - Random Forest
- Mixture Modeling (Unsupervised Learning)
  - Univariate
  - Bivariate
  - With covariates (Mixture of Experts)
]

---

class: inverse, middle, center

# Imaging: Reference Regions

---

# Reference Regions

.large[
A common issue with statistical analyses of numeric summaries derived from imaging data is normalization to a reference region. 

Examples:

- Volumetric MRI: regional (e.g. hippocampal) volume relative to Intra-Cranial Volume (ICV)
- Amyloid PET: cortical-to-cerebellum Standardized Uptake Value Ratio (SUVr)

(Also an issue in non-imaging data, e.g. ratio of CSF `\(\textrm{A}\beta_{1-42}\)` to `\(\textrm{A}\beta_{1-40}\)`)
]

---

# Beware the "Ratio Fallacy" and "Spurious Correlation"
.large[
&gt; Spurious correlation refers to the correlation between indices that have a common component. A 'per ratio' standard is based on a biological measurement adjusted for some physical measurement by division. Renowned statisticians and biologists (Pearson, Neyman and Tanner) have warned about the problems in interpretation that ratios cause. This warning has been largely ignored. The consequences of using a single ratio as either the dependent or one of the independent variables in a multiple-regression analysis are described. It is shown that the use of ratios in regression analyses can lead to incorrect or misleading inferences. A recommendation is made that the use of ratios in regression analyses be avoided.

Kronmal (1993) "Spurious correlation and the fallacy of the ratio standard revisited". *Journal of the Royal Statistical Society: Series A*.

Examples abound: %ICV, SUVR, BMI, ...
]

---

# Ratio as Dependent Variable in Regression




``` r
lm_fit1_sex &lt;- lm(I(Hippocampus/ICV*100) ~ Sex, data=dd2)
```



|Coefficient | Estimate| Std. Error| t value|Pr(&gt;&amp;#124;t&amp;#124;) |
|:-----------|--------:|----------:|-------:|:------------------|
|(Intercept) |     0.49|          0|   150.7|&lt;0.001             |
|SexFemale   |     0.04|          0|     9.3|&lt;0.001             |



- Signficant association between Sex and Hippocampal Volume (%ICV)!
- (This linear model is essential just a two-sample t-test)
- Model fit to ADNI CN

---

# ICV as Covariate Instead of Denominator


``` r
lm_fit2_sex &lt;- lm(Hippocampus ~ ICV + Sex, data=dd2)
```



|Coefficient | Estimate| Std. Error| t value|Pr(&gt;&amp;#124;t&amp;#124;) |
|:-----------|--------:|----------:|-------:|:------------------|
|(Intercept) |     4383|        362|   12.12|&lt;0.001             |
|ICV         |        0|          0|    9.23|&lt;0.001             |
|SexFemale   |       37|         73|    0.51|0.61               |



- No association between Sex and Hippocampal Volume ( `\(\text{mm}^3\)` )!
- The **spurious association** in first model is driven by denominator (ICV), not numerator (hippocampal volume)

---

# Hippocampus vs ICV and Sex

.pull-leftWider[
&lt;img src="fluid_fig/hipp_sex_scatter-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;
]

.pull-rightNarrower[

- For a given ICV, there is not much difference between sexes
- These linear fits are more flexible than model above (two slopes for ICV vs one)

]

---

# Hippocampal volume ( `\(\textrm{mm}^3\)`, %ICV, and model-adjusted)

&lt;img src="fluid_fig/hipp_sex_box-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Hippocampal volume ( `\(\textrm{mm}^3\)`, %ICV, and model-adjusted)

.pull-leftWider[

&lt;img src="fluid_fig/hipp_adj_sex_scatter-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

]

.pull-rightNarrower[

- Model: 
.small[
`\(Y_i=\beta_0 + \beta_{\textrm{ICV}}\textrm{ICV}_i  + \beta_{\textrm{Male}}\textrm{Male}_i + \varepsilon_i\)`
]
- Adjusted volume: `\(Y_i - \hat{\beta}_{\textrm{ICV}}\textrm{ICV}_i\)` 
- Adjustment removes slope for ICV

]

---

class: inverse, middle, center

# Centiloids

---

# Centiloids

.large[

- Centiloids have become the industry standard standardized measure of amyloid PET (Klunk et al., 2015)
- Helpful in multi-site studies where different amyloid PET tracers might be utilized.
- What is a centiloid?

]

---

# What is a centiloid? Step 1: PiB SUVr to CL

.large[

- The original/base PiB SUVr to centiloid map:
- 1.009 PiB SUVr (mean in young controls) `\(\rightarrow\)` 0 CL
- 2.076 PiB SUVr (mean in AD cases) `\(\rightarrow\)` 100 CL
- For other PiB SUVr values, draw the line from (1.009 SUVr, 0 CL) to (2.076 SUVr, 100 CL)
- CL = 100 `\(\times\)` (PiB SUVr - 1.009)/1.067 (Klunk et al., 2015)

]

---

# What is a centiloid? Step 1: PiB SUVr to CL

&lt;img src="fluid_fig/unnamed-chunk-21-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# What is a centiloid? Step 2: Other SUVr to PiB SUVr

.large[

- Step 2 requires *paired* data: each individuals scanned with PiB and the other tracer
- Linear regression (ordinary least squares) is used to create linear transformation
- Transformed data is then mapped to CL using map on prior slide

]

---

# What is a centiloid? Step 2: Other SUVr to PiB SUVr

&lt;img src="./images/rowe2017.png" width="75%"  style="display: block; margin: auto;" /&gt;

`\(\textrm{SUVR}_{\textrm{FBB}} = 0.61 \times  \textrm{SUVR}_{\textrm{PiB}} + 0.39\)`

Rowe et al. (2017)

---

# Distribution mapping

&lt;img src="./images/properzi2019.png" width="75%"  style="display: block; margin: auto;" /&gt;

Properzi et al. (2019)

???

Another approach to normalizing data from different tracers (or fluid assays for that matter) is to map distributions.

If we only have unpaired data from similar populations, we can map one to other under the assumption that their distributions should have similar shape.

---

# Emprical Cumulative Distribution Function (ECDF)

&lt;img src="fluid_fig/unnamed-chunk-24-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

CDFs are useful one-to-one maps

Underlie random number generators. A random number from a uniform 0-1 distribution can be mapped to any other distribution by the inverse CDF.

In this case we can map PiB SUVRs to florbetapir SUVRs or vice versa

But it only works if samples are similar, and we believe both tracers would sort the samples similarly.

---

# Weighted ECDFs to correct for sampling differences

N (%) per diagnosis and tracer:



|         |PiB        |Florbetapir |
|:--------|:----------|:-----------|
|CN       |20 (21.5%) |498 (38.5%) |
|MCI      |49 (52.7%) |588 (45.4%) |
|Dementia |24 (25.8%) |209 (16.1%) |



Inverse proportion weights:



|         | PiB| Florbetapir|
|:--------|---:|-----------:|
|CN       | 4.7|         2.6|
|MCI      | 1.9|         2.2|
|Dementia | 3.9|         6.2|



???

similar methodology underlies CDC growth charts that provide height and weight 
percentiles per age 

---

# Weighted ECDFs to correct for sample differences


``` r
# Record the sampling adjustment weights in the data
dd &lt;- dd %&gt;% mutate(
  wt = case_when(
    DX == 'CN' &amp; Tracer == 'PiB' ~ invproptab['CN', 'PiB'],
    DX == 'MCI' &amp; Tracer == 'PiB' ~ invproptab['MCI', 'PiB'],
    DX == 'Dementia' &amp; Tracer == 'PiB' ~ invproptab['Dementia', 'PiB'],
    DX == 'CN' &amp; Tracer == 'Florbetapir' ~ invproptab['CN', 'Florbetapir'],
    DX == 'MCI' &amp; Tracer == 'Florbetapir' ~ invproptab['MCI', 'Florbetapir'],
    DX == 'Dementia' &amp; Tracer == 'Florbetapir' ~ invproptab['Dementia', 'Florbetapir']
  ))
```

---

# Weighted ECDFs to correct for sample differences




``` r
# Create adjusted ECDF functions (mapping SUVRs to Cumulative Probabilities)
# Hmisc::wtd.Ecdf returns a data.frame evaluating the ECDF at each observed value
PiB.ecdf &lt;- with(subset(dd, Tracer == 'PiB'), ecdf.func(SUVR, weights=wt))
Fbp.ecdf &lt;- with(subset(dd, Tracer == 'Florbetapir'), ecdf.func(SUVR, weights=wt))

# Create adjusted **inverse** ECDF functions 
# mapping Cumulative Probabilities (0 to 1 scale) to SUVRs
PiB.inv.ecdf &lt;- with(subset(dd, Tracer == 'PiB'), 
 inv.ecdf(SUVR, weights=wt))
Fbp.inv.ecdf &lt;- with(subset(dd, Tracer == 'Florbetapir'),  
 inv.ecdf(SUVR, weights=wt))
```

---

# Weighted ECDFs to correct for sample differences


``` r
dd &lt;- dd %&gt;% mutate(
  `Adjusted cumulative probability` = case_when( # 
    Tracer == 'PiB' ~ PiB.ecdf(SUVR),
    Tracer == 'Florbetapir' ~ Fbp.ecdf(SUVR)),
  `Adjusted Z-score` = qnorm(`Adjusted cumulative probability`), # adjusted z-scores
  `Florbetapir to PiB adjusted SUVR` = case_when(
    Tracer == 'Florbetapir' ~ PiB.inv.ecdf(Fbp.ecdf(SUVR))),
  `PiB to Florbetapir adjusted SUVR` = case_when(
    Tracer == 'PiB' ~ Fbp.inv.ecdf(PiB.ecdf(SUVR))),
  CL = case_when(
    Tracer == 'PiB' ~ 100*(SUVR - 1.009)/1.067)) %&gt;%
  arrange(Tracer, SUVR)
```

---

# Weighted ECDFs to correct for sample differences

&lt;img src="fluid_fig/weighted-ecdfs-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

* solid line is un-adjusted ECDF; dashed line is ECDF adjusted for sample differences

---

# Centiloids vs ADNI Adjusted Percentiles

&lt;img src="fluid_fig/unnamed-chunk-31-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Densities for estimated and actual PiB SUVRs

&lt;img src="fluid_fig/pib-densities-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

???

Not perfectly matched due to different sample characteristics. To be expected.

Recall PiB had less representation of CU and Florbetapir had more with dementia

---

# Densities by diagnosis

&lt;img src="fluid_fig/pib-densities-dx-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Densities for weighted ECDF derived z-scores

&lt;img src="fluid_fig/z-score-densities-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

Useful for analysis, as in Li et al. (2019)

???

Useful for analysis

---

# Using the ECDF derived z-scores for analysis

.pull-left[

&lt;img src="./images/li2019.png" width="95%"  style="display: block; margin: auto;" /&gt;

]

.pull-right[

* ECDF derived z-scores were used in model (assuming Gaussian residuals)
* Posterior estimates (on z-score scale) then back transformed to cumulative probabilities
* Natural interpretation:
  * 0 (most healthy) to 1 (most severe) 
  * Comparable across different measures

(Li et al., 2019)

]

---

# Validation on a holdout set with both tracers (FBB `\(\rightarrow\)` PiB)

&lt;img src="fluid_fig/unnamed-chunk-33-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

`\(^*\)` Using PiB = (Florbetapir - 0.502)/0.536 from Navitsky et al. (2018)

???

We held out a random sample of 10 participants with both PiB and FBB

Note that the linear mapping from Navitsky is estimating PiB values outside of 
the observed range of actual PiB values (near 0.75). 

This will never happen with ECDF mapping.

---

# Validation on a holdout set with both tracers (PiB `\(\rightarrow\)` FBB)

&lt;img src="fluid_fig/unnamed-chunk-34-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

`\(^*\)` Using Florbetapir = PiB `\(\times\)` 0.536 + 0.502 from Navitsky et al. (2018)

???

This type of approach could also be useful for standardizing fluid assays

---

# Validation on a holdout set with both tracers (FBB `\(\rightarrow\)` PiB)

&lt;img src="fluid_fig/unnamed-chunk-35-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

`\(^*\)` Using PiB = (Florbetapir - 0.503)/0.497 from Royse et al. (2021)

???

---

# Validation on a holdout set with both tracers (PiB `\(\rightarrow\)` FBB)

&lt;img src="fluid_fig/unnamed-chunk-36-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

`\(^*\)` Using Florbetapir = PiB `\(\times\)` 0.497 + 0.503 from Royse et al. (2021)

???

dashed line is FBP threshold of 1.11

---

# Where is linear map struggling?

.pull-left[

&lt;img src="./images/Royse_2021_Fig2.png" width="80%"  style="display: block; margin: auto;" /&gt;

]

.pull-right[

&lt;img src="./images/Royse_2021_Fig2_zoom.png" width="70%"  style="display: block; margin: auto;" /&gt;

]

From Royse et al. (2021)

---

class: inverse, middle, center

# Longitudinal Data

---

# Repeated Measures/Longitudinal Data

.large[

* In Alzheimer's Disease (AD) studies we typically assess study participants **longitudinally** or **repeatedly** over time
* This gives rise to **serial observations** for each participant at various time points post baseline
* We cannot treat these serial observations as if they came from different people
* Our analysis methods must account for **within-subject correlation**
* There are specialized statistical methods to help accommodate many varieties of **correlated** or **clustered** data
* We will explore **longitudinal data analysis** approaches commonly employed in AD clinical trials
* We will demonstrate these methods on a **simulated** clinical trial dataset 

]

---

# Repeated Measures/Longitudinal Data



```
Linear mixed-effects model fit by REML
  Data: ADNIMERGE::adnimerge %&gt;% mutate(Sex = factor(PTGENDER, levels = c("Male",      "Female"))) 
  Subset: M &lt;= 24 &amp; DX.bl == "AD" 
   AIC  BIC logLik
  7730 7771  -3857

Random effects:
 Formula: ~M | RID
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev Corr  
(Intercept) 5.98   (Intr)
M           0.38   0.53  
Residual    3.36         

Fixed effects:  ADAS11 ~ Sex + center(AGE) + I(Years.bl * 12) 
                 Value Std.Error  DF t-value p-value
(Intercept)       19.3      0.44 799      44   0.000
SexFemale          0.3      0.65 407       0   0.624
center(AGE)        0.1      0.04 407       2   0.039
I(Years.bl * 12)   0.4      0.02 799      16   0.000
 Correlation: 
                 (Intr) SexFml c(AGE)
SexFemale        -0.658              
center(AGE)      -0.048  0.088       
I(Years.bl * 12)  0.139 -0.001  0.011

Standardized Within-Group Residuals:
  Min    Q1   Med    Q3   Max 
-2.74 -0.44 -0.04  0.41  2.47 

Number of Observations: 1210
Number of Groups: 410 
```

---

# Simulating a hypothetical clinical trial `\(\ldots\)`

.large[

* Two groups: placebo vs active (hypothetical)
* `\(n=200\)` mild to moderate dementia subjects per group
* Alzheimer's Disease Assessment Scale (ADAS-Cog) assessed at 0, 6, 12, 18 months
* Placebo group behaves like ADNI participants
* Weak effects for age and sex (based on ADNI pilot estimates)
* A treatment which slows ADAS-Cog progression by 12%
* Typical attrition (about 30%)

]

---

# Simulating a hypothetical clinical trial `\(\ldots\)`

.large[

Simulation **reverses** the usual process of statistical modeling/estimation

* Model fitting: Data + Model `\(\rightarrow\)` Parameter Estimates
* Model simulation: Model + Parameter Estimates `\(\rightarrow\)` Pseudo Data
* Given a reasonable model, everything can be simulated: mean, variance, missingness, etc.
* CAUTION: Simulations can only provide information about **models**, but they cannot 
provide information about **reality**. **Real data** is required for the latter.

]

---

# Simulating a hypothetical clinical trial `\(\ldots\)`

These are all estimates required:


``` r
# fixed effects parameters estimated from ADNI
Beta &lt;- c(
   '(Intercept)'=20 , # mean ADAS at baseline
        'female'=0.1, # worse scores for females
         'age_c'=0.1, # worse change for older at baseline (age mean centered)
         'month'=0.4, # worse per month post baseline
  'month:active'=-0.05) # improvement per month with treatment

# standard deviations for random effects
sigma_random_intercept &lt;- 6.0
sigma_random_slope &lt;- 0.37
sigma_residual &lt;- 3.3

# other design parameters
months &lt;- c(0, 6, 12, 18)
n &lt;- 200 # per group
attrition_rate &lt;- 0.40/18 # approx per month
```




---

# The pseudo data snapshot



| id| month| active| female| age| censor| ran.intercept| ran.slope| age_c|Female | residual| ADAS11|
|--:|-----:|------:|------:|---:|------:|-------------:|---------:|-----:|:------|--------:|------:|
|  1|     0|      1|      0|  66|     69|           8.7|      0.37|  -8.7|Male   |     0.43|     28|
|  1|     6|      1|      0|  66|     69|           8.7|      0.37|  -8.7|Male   |    -1.55|     31|
|  1|    12|      1|      0|  66|     69|           8.7|      0.37|  -8.7|Male   |     2.36|     39|
|  1|    18|      1|      0|  66|     69|           8.7|      0.37|  -8.7|Male   |     3.66|     44|
|  2|     0|      1|      1|  88|     51|          -1.7|     -0.17|  13.1|Female |    -1.02|     19|
|  2|     6|      1|      1|  88|     51|          -1.7|     -0.17|  13.1|Female |     5.49|     26|



---

# The pseudo data: Baseline characteristics



|             |placebo (N=200) |active (N=200)  |Total (N=400)   |p value |
|:------------|:---------------|:---------------|:---------------|:-------|
|Female       |                |                |                |0.764   |
|-  Female    |106 (53.0%)     |103 (51.5%)     |209 (52.2%)     |        |
|-  Male      |94 (47.0%)      |97 (48.5%)      |191 (47.8%)     |        |
|age          |                |                |                |0.689   |
|-  Mean (SD) |74.716 (7.936)  |75.036 (8.039)  |74.876 (7.979)  |        |
|-  Range     |53.884 - 97.940 |57.228 - 96.349 |53.884 - 97.940 |        |
|ADAS11       |                |                |                |0.386   |
|-  Mean (SD) |20.725 (7.002)  |20.135 (6.576)  |20.430 (6.790)  |        |
|-  Range     |5.000 - 44.000  |3.000 - 37.000  |3.000 - 44.000  |        |



---

# The pseudo data: Spaghetti plot

&lt;img src="fluid_fig/spaghetti_plot-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Basic longitudinal summaries of ADAS11
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; group &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; month &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; mean &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; sd &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; lower95 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; upper95 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; min &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; max &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;vertical-align: middle !important;" rowspan="4"&gt; placebo &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 44 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 24 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 43 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 163 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 51 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 149 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 29 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 28 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 31 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 55 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;vertical-align: middle !important;" rowspan="4"&gt; active &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 19 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 37 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 39 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 158 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 45 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 142 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 28 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 54 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



---

# Mean ADAS

&lt;img src="fluid_fig/meanplot-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Basic longitudinal summaries of ADAS11 _change_

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; group &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; month &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; mean &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; sd &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; lower95 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; upper95 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; min &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; max &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;vertical-align: middle !important;" rowspan="4"&gt; placebo &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -14 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 19 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 163 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 24 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 149 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -11 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 28 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;vertical-align: middle !important;" rowspan="4"&gt; active &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 158 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -11 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 142 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -15 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



---

# Mean ADAS change (95% CI)

&lt;img src="fluid_fig/meanchplot-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

class: inverse, middle, center

# `\(t\)`-test

---

# Two sample `\(t\)`-test of mean change at month 18 

## (completers analysis)



.large[

* Difference between group means is
    8.34 - 6.51  = 1.83 
* (pooled) standard deviation is 8.11 
*  `\(t = \frac{1.83}{8.11\sqrt{\frac{1}{149} + \frac{1}{142}}} =  1.92\)`
* 149 + 142 - 2 = 289 _degrees of freedom_

]

---

# `\(t\)`-test


```

	Two Sample t-test

data:  ADAS11.ch by group
t = 1.923, df = 289, p-value = 0.0555
alternative hypothesis: true difference in means between group placebo and group active is not equal to 0
95 percent confidence interval:
 -0.0429371  3.6993319
sample estimates:
mean in group placebo  mean in group active 
              8.34228               6.51408 
```

---

# The `\(t_{289}\)`-distribution

`\(p\)`-value is area under curve for `\(|x|&gt;1.92\)`, the value of the test statistic in this case. This area is depicted in blue here (area = `\(p\)` = 0.055).

&lt;img src="fluid_fig/unnamed-chunk-47-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

class: inverse, middle, center

# Regression

---

# Regression analysis

.large[

* **Regression** generally refers to a relationship between variables that is estimated by data
* **Ordinary Least Squares** regression, for example, describes a linear relationship between two continuous variables that is estimated by the line that minimizes the sum of squared "residuals"
* **predictor/covariate** `\(\rightarrow\)` **response/outcome**
* **Residuals** are the differences between observations and values predicted by the regression

]

---

# Other types of regression

.large[

* **General linear models** can add multiple **covariates/predictors**
* **Generalized linear models** can accommodate other types of **outcome/response** variables (e.g. logistic regression can accommodate binary outcome variables)
* **Mixed-effects models** mix **random effects** with the standard **fixed effects** to account for complex correlation structures

All regression models share the common theme of estimating the best fit relationship between
**outcome/response** variables and **covariates/predictors** under some assumptions.

]

---

class: inverse, middle, center

# ANCOVA

---

# ANalysis of COVAriance (ANCOVA) for "pre-post" data

* Very common for two groups, and **one** post- assessment
* `\(\textrm{ADAS}_{i1}\)`: baseline or pre- observation for subject `\(i\)`, `\(i=1,\dots,200\)`
* `\(\textrm{ADAS}_{i2}\)`: followup or post- observation for subject `\(i\)`, `\(i=1,\dots,200\)`
* `\(\textrm{Active}_i\)`: treatment group indicator (e.g. 1 if active, 0 if placebo)
* _ANCOVA I_: `\(\textrm{ADAS}_{i2} = \beta_0 + \textrm{Active}_i\beta_1 + \textrm{ADAS}_{i1}\beta_2 + \varepsilon_i\)`
  * `\(\beta_0\)` is the _intercept_
  * `\(\beta_1\)` is the estimate of interest: group difference at 18 months
  * `\(\beta_2\)` controls for baseline ADAS
  * `\(\varepsilon_i\)` is residual error
* _ANCOVA II_: `\(\textrm{ADAS}_{i2} = \beta_0 + \textrm{Active}_i\beta_1 + \textrm{ADAS}_{i1}^*\beta_2 + \textrm{Active}_i\textrm{ADAS}_{i1}^*\beta_3 + \varepsilon_i\)`
  * `\(\beta_3\)` controls for interaction of treatment assignment and baseline ADAS
  * _Need to mean center baseline covariates_: `\(\textrm{ADAS}_{i1}^* = \textrm{ADAS}_{i1} - \bar{Y}_{\cdot0}\)`
  * More efficient (statistical power)
* "ANCOVA I model also fully efficient if the randomization probability to the treatment arm is 0.5 or when the covariance between the baseline and follow-up responses is the same for both the treatment and control groups"
  
Yang and Tsiatis (2001)

---

# ANCOVA I for effect of treatment on ADAS11 at 18 months


```

Call:
lm(formula = ADAS11.ch ~ active + ADAS11.m0, data = trial_mmrm)

Residuals:
    Min      1Q  Median      3Q     Max 
-18.113  -4.546  -0.488   4.336  24.358 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  10.0163     0.7316   13.69  &lt; 2e-16 ***
active       -1.0217     0.4270   -2.39    0.017 *  
ADAS11.m0    -0.2353     0.0321   -7.32  5.2e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.7 on 969 degrees of freedom
Multiple R-squared:  0.0562,	Adjusted R-squared:  0.0543 
F-statistic: 28.9 on 2 and 969 DF,  p-value: 6.71e-13
```

---

# ANCOVA II for effect of treatment on ADAS11 at 18 months


```

Call:
lm(formula = ADAS11.ch ~ active * center(ADAS11.m0), data = trial_mmrm)

Residuals:
    Min      1Q  Median      3Q     Max 
-18.318  -4.453  -0.436   4.304  24.069 

Coefficients:
                         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                5.2199     0.3000   17.40  &lt; 2e-16 ***
active                    -1.0203     0.4269   -2.39    0.017 *  
center(ADAS11.m0)         -0.2739     0.0445   -6.15  1.1e-09 ***
active:center(ADAS11.m0)   0.0805     0.0643    1.25    0.211    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.7 on 968 degrees of freedom
Multiple R-squared:  0.0577,	Adjusted R-squared:  0.0548 
F-statistic: 19.8 on 3 and 968 DF,  p-value: 1.92e-12
```

---

# ANCOVA II with more covariates

```

Call:
lm(formula = ADAS11.ch ~ active * center(ADAS11.m0) + female + 
    age_c, data = trial_mmrm)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.306  -4.497  -0.398   4.314  23.719 

Coefficients:
                         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                4.9185     0.3770   13.05  &lt; 2e-16 ***
active                    -1.0587     0.4254   -2.49    0.013 *  
center(ADAS11.m0)         -0.2869     0.0446   -6.44  1.9e-10 ***
female                     0.5765     0.4266    1.35    0.177    
age_c                      0.0758     0.0269    2.81    0.005 ** 
active:center(ADAS11.m0)   0.0901     0.0642    1.40    0.161    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.6 on 966 degrees of freedom
Multiple R-squared:  0.0671,	Adjusted R-squared:  0.0623 
F-statistic: 13.9 on 5 and 966 DF,  p-value: 3.87e-13
```

---

# ANCOVA summary

.large[

* Familiar, simple, powerful framework
* ANCOVA more powerful/efficient than `\(t\)`-test
* Both are inherently **complete case** analyses!
* With missing data, not intention-to-treat (ITT) analysis
* Might be biased and/or inefficient (low power) with missing data

]

---

class: inverse, middle, center

# Two-stage models

---

# Two-stage models

.large[

* _Subject-specific_ longitudinal profiles can often be modeled with simple linear regression
* This leads to the 2-stage model:
  
  * _Stage 1_: Linear regression model for each subject separately
  * _Stage 2_: Model subject-specific regression coefficients with covariates of interest
  
* However, this is **NOT** a recommend analysis approach, but rather a means to introduce mixed-effect models.

]

---

# Two-stage model example

.large[

* _Stage 1_: `\(\textrm{ADAS}_{ij} = \beta_{0i} + t_{ij}\beta_{1i} + \varepsilon_i\)` for subject `\(i\)` at time `\(t_{ij}\)`
* Provides estimates of subject-specific intercepts, `\(\hat\beta_{0i}\)` and slopes `\(\hat\beta_{0i}\)`
* `\(\varepsilon_i \sim \mathcal{N}(0,\sigma^2_iI_{n_i})\)` estimates _within_-subject variability
* _Between_-subject variability can now be modeled by treating `\(\hat\beta_i\)` as "response variables"
* _Stage 2_: `\(\hat\beta_{1i} = X_i\beta + \varepsilon'_i\)`

]

---

# Stage 1 models of simulated trial

&lt;img src="fluid_fig/trial_stage1_plot-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Stage 1 model of simulated trial



| id| beta.(Intercept)| beta.month| sigma| active|group   | age_c| female|
|--:|----------------:|----------:|-----:|------:|:-------|-----:|------:|
|  1|               27|       0.93|   1.4|      1|active  |  -8.7|      0|
|  2|               19|       0.70|   4.0|      1|active  |  13.1|      1|
|  3|                8|         NA|   NaN|      0|placebo | -17.3|      1|
|  4|                9|      -0.17|   NaN|      1|active  |   8.5|      0|
|  5|               20|       0.17|   3.3|      1|active  | -15.3|      1|
|  6|               18|         NA|   NaN|      1|active  |   4.2|      1|



???

NAs are from too few observations due to missing data

---

# Stage 1 model of simulated trial


```

Call:
lm(formula = ADAS11 ~ month, data = trial_obs, subset = id == 
    1)

Residuals:
   1    2    3    4 
 0.9 -1.7  0.7  0.1 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   27.100      1.212   22.35    0.002 **
month          0.933      0.108    8.64    0.013 * 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.4 on 2 degrees of freedom
Multiple R-squared:  0.974,	Adjusted R-squared:  0.961 
F-statistic: 74.7 on 1 and 2 DF,  p-value: 0.0131
```

---

# Stage 2 model of simulated trial

&lt;img src="fluid_fig/trial_plot_stage2-1.svg" width="100%"  style="display: block; margin: auto;" /&gt;

---

# Stage 2 model of simulated trial


```

Call:
lm(formula = beta.month ~ female + age_c + active, data = trial_stage1)

Residuals:
   Min     1Q Median     3Q    Max 
-2.508 -0.313  0.057  0.353  1.846 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.38692    0.05039    7.68  1.6e-13 ***
female      -0.00358    0.05694   -0.06     0.95    
age_c        0.00378    0.00356    1.06     0.29    
active      -0.03054    0.05688   -0.54     0.59    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.54 on 356 degrees of freedom
  (40 observations deleted due to missingness)
Multiple R-squared:  0.0039,	Adjusted R-squared:  -0.00449 
F-statistic: 0.465 on 3 and 356 DF,  p-value: 0.707
```

---

# Two-stage models

.large[

* In contrast to ANCOVA and `\(t\)`-test, two-stage models allow all randomized subject with at least one followup to be included into analysis ("*modified intention-to-treat*")
* However, second stage models ignore the variability/uncertainty of the slope estimates from the first stage
* This means that `\(p\)`-values from second stage might be smaller than they should be and Type I error could be inflated

]



---

# Summary of considerations for imaging data

.large[

* Avoid using ratios whenever possible. Better to include "denominators" as covariates in regression models.
* The Empirical Cumulative Distribution Function (ECDF) is a useful non-parametric tool for standardization.
  * Can handle non-linear relationships
  * Percentiles are intuitive and familiar
  * ECDF derived maps will not produce out-of-range estimates
  * Percentiles can be mapped to standard normal `\(z\)`-scores to facilitate modelling
* Introduction to longitudinal data analysis
  * Plots, summaries, `\(t\)`-test, regression, ANCOVA, two-stage models

]

---

# References (1/3)

Benaglia, T., D. Chauveau, D. R. Hunter, and D. Young (2009).
"mixtools: An R Package for Analyzing Finite Mixture Models". In:
_Journal of Statistical Software_ 32.6, pp. 1-29. URL:
[http://www.jstatsoft.org/v32/i06/](http://www.jstatsoft.org/v32/i06/).

Breiman, L. (2001). "Random forests". In: _Machine learning_ 45.1, pp.
5-32.

Davidian, M. and P. D. Haaland (1990). "Regression and calibration with
nonconstant error variance". In: _Chemometrics and Intelligent
Laboratory Systems_ 9.3, pp. 231-248.

Haaland, P., D. Samarov, and E. McVey (2011). _calibFit: Statistical
models and tools for assay calibration_. R package version 2.1.0. URL:
[https://CRAN.R-project.org/package=calibFit](https://CRAN.R-project.org/package=calibFit).

Hothorn, T., P. Buehlmann, S. Dudoit, A. Molinaro, and M. Van Der Laan
(2006). "Survival Ensembles". In: _Biostatistics_ 7.3, pp. 355-373.

Hothorn, T., K. Hornik, and A. Zeileis (2006). "Unbiased Recursive
Partitioning: A Conditional Inference Framework". In: _Journal of
Computational and Graphical Statistics_ 15.3, pp. 651-674.

Klunk, W. E., R. A. Koeppe, J. C. Price, T. L. Benzinger, M. D. Devous
Sr, W. J. Jagust, K. A. Johnson, C. A. Mathis, D. Minhas, M. J.
Pontecorvo, et al. (2015). "The Centiloid Project: standardizing
quantitative amyloid plaque estimation by PET". In: _Alzheimer's &amp;
dementia_ 11.1, pp. 1-15.

---

# References (2/3)

Kronmal, R. A. (1993). "Spurious correlation and the fallacy of the
ratio standard revisited". In: _Journal of the Royal Statistical
Society: Series A (Statistics in Society)_ 156.3, pp. 379-392.

Li, D. et al. (2019). "Bayesian latent time joint mixed effect models
for multicohort longitudinal data". In: _Statistical methods in medical
research_ 28.3, pp. 835-845.

Murphy, K. et al. (2020). "Gaussian Parsimonious Clustering Models with
Covariates and a Noise Component". In: _Advances in Data Analysis and
Classification_ 14.2, pp. 293-325. DOI:
[10.1007/s11634-019-00373-8](https://doi.org/10.1007%2Fs11634-019-00373-8).
URL:
[https://doi.org/10.1007/s11634-019-00373-8](https://doi.org/10.1007/s11634-019-00373-8).

Murphy, K. et al. (2022). _\texttt\textupMoEClust: Gaussian
Parsimonious Clustering Models with Covariates and a Noise Component_.
\textsfR package version 1.5.0. URL:
[https://cran.r-project.org/package=MoEClust](https://cran.r-project.org/package=MoEClust).

Navitsky, M. et al. (2018). "Standardization of amyloid quantitation
with florbetapir standardized uptake value ratios to the Centiloid
scale". In: _Alzheimer's &amp; Dementia_ 14.12, pp. 1565-1571.

Properzi, M. J. et al. (2019). "Nonlinear Distributional Mapping
(NoDiM) for harmonization across amyloid-PET radiotracers". In:
_Neuroimage_ 186, pp. 446-454.

---

# References (3/3)

Properzi, M. J. et al. (2019). "Nonlinear Distributional Mapping
(NoDiM) for harmonization across amyloid-PET radiotracers". In:
_Neuroimage_ 186, pp. 446-454.

Robin, X. et al. (2011). "pROC: an open-source package for R and S+ to
analyze and compare ROC curves". In: _BMC Bioinformatics_ 12, p. 77.

Rowe, C. C. et al. (2017). "18 F-Florbetaben PET beta-amyloid binding
expressed in Centiloids". In: _European journal of nuclear medicine and
molecular imaging_ 44.12, pp. 2053-2059.

Royse, S. K. et al. (2021). "Validation of amyloid PET positivity
thresholds in centiloids: a multisite PET study approach". In:
_Alzheimer's Research &amp; Therapy_ 13.1, pp. 1-10.

Yang, L. et al. (2001). "Efficiency study of estimators for a treatment
effect in a pretest-posttest trial". In: _The American Statistician_
55.4, pp. 314-321.

Youden, W. J. (1950). "Index for rating diagnostic tests". In: _Cancer_
3.1, pp. 32-35.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized-light",
"highlightLanguage": ["r", "css", "yaml"],
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
